{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 01: SoftwareIntro\n",
    "The purpose of this homework is to guide you through opening the programs you've downloaded through Neurodebian and getting familiar with some of their basic functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download example data\n",
    "\n",
    "We will first download some practice MRI data from open science framework. While this is not a common outlet for scientists to share their MR imaging datasets, this is a useful repository for files that are larger than we would want to put on github and are in a simplified storage format for teaching and demonstrations like this one.\n",
    "\n",
    "Download from osf: https://osf.io/bprq5/\n",
    "\n",
    "After clicking on the link, you should see the download __lab01_images.tar.gz__ appear in your browser downloads. We will practice using the bash terminal to navigate to the Downloads folder, and to copy and unpack the download to our lab directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#^the %% symbols tell the notebook to execute bash commands\n",
    "cd ~/Downloads  #change directories to the Downloads folder, the ~ is shorthand for \"home directory\"\n",
    "ls  #list the files there"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%bash\n",
    "pwd  #tell me where I am again?\n",
    "#when I move cells, I moved back to the 01-Lab directory,but I can still move the file to here\n",
    "mv /home/brain/Downloads/lab01_images.tar.gz ./lab01_images.tar.gz"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%bash\n",
    "ls #is it here?\n",
    "#unpack a tar file: tar -zxvf tar-archive-name.tar.gz\n",
    "tar -zxvf lab01_images.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img\n",
      "lab01_images.tar.gz\n",
      "osfshare\n",
      "SoftwareIntro.ipynb\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls  #now osfshare is a directory, we will go into this directory to practice opening MR images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-97-T1w_brain_mask.nii.gz\n",
      "sub-97-T1w_brain.nii.gz\n",
      "sub-97-T1w_defaced.nii.gz\n",
      "sub-97_task-flanker_bold.nii.gz\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd osfshare\n",
    "ls  #list contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview the data directly in jupyter\n",
    "\n",
    "We'll first get used to terms about the spatial orientation of an MR image. We'll use a viewer that is part of nipype, which is a collection of neuroimaging tools written in the python programming language. \n",
    "\n",
    "More information about the NiftiWidget can be found here: http://nipy.org/niwidgets/examples.html. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Import an example image that comes with the package__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f180ab58fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beedf47479d9437ea3f074e82a1674ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(IntSlider(value=45, continuous_update=False, description='x', max=90), IntSlider(value=54, continuous_update=False, description='y', max=108), IntSlider(value=45, continuous_update=False, description='z', max=90), Dropdown(description='colormap', options=('viridis', 'Accent', 'Blues', 'BrBG', 'BuGn', 'BuPu', 'CMRmap', 'Dark2', 'GnBu', 'Greens', 'Greys', 'OrRd', 'Oranges', 'PRGn', 'Paired', 'Pastel1', 'Pastel2', 'PiYG', 'PuBu', 'PuBuGn', 'PuOr', 'PuRd', 'Purples', 'RdBu', 'RdGy', 'RdPu', 'RdYlBu', 'RdYlGn', 'Reds', 'Set1', 'Set2', 'Set3', 'Spectral', 'Vega10', 'Vega20', 'Vega20b', 'Vega20c', 'Wistia', 'YlGn', 'YlGnBu', 'YlOrBr', 'YlOrRd', 'afmhot', 'autumn', 'binary', 'bone', 'brg', 'bwr', 'cool', 'coolwarm', 'copper', 'cubehelix', 'flag', 'gist_earth', 'gist_gray', 'gist_heat', 'gist_ncar', 'gist_rainbow', 'gist_stern', 'gist_yarg', 'gnuplot', 'gnuplot2', 'gray', 'hot', 'hsv', 'jet', 'nipy_spectral', 'ocean', 'pink', 'prism', 'rainbow', 'seismic', 'spectral', 'spring', 'summer', 'tab10', 'tab20', 'tab20b', 'tab20c', 'terrain', 'winter'), value='viridis'), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#import the widget from the niwidgets package\n",
    "from niwidgets import NiftiWidget   \n",
    "\n",
    "#import example data that comes with niwidgets\n",
    "from niwidgets import examplet1   \n",
    "\n",
    "#commands that open the image in the widget\n",
    "#test_widget = NiftiWidget(examplet1)\n",
    "test_widget.nifti_plotter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Import our own image__ <br>\n",
    "What are some similarities and differences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f180ac22400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b23d9738e744ddc87c621ca403c25d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(IntSlider(value=119, continuous_update=False, description='x', max=239), IntSlider(value=127, continuous_update=False, description='y', max=255), IntSlider(value=127, continuous_update=False, description='z', max=255), Dropdown(description='colormap', options=('viridis', 'Accent', 'Blues', 'BrBG', 'BuGn', 'BuPu', 'CMRmap', 'Dark2', 'GnBu', 'Greens', 'Greys', 'OrRd', 'Oranges', 'PRGn', 'Paired', 'Pastel1', 'Pastel2', 'PiYG', 'PuBu', 'PuBuGn', 'PuOr', 'PuRd', 'Purples', 'RdBu', 'RdGy', 'RdPu', 'RdYlBu', 'RdYlGn', 'Reds', 'Set1', 'Set2', 'Set3', 'Spectral', 'Vega10', 'Vega20', 'Vega20b', 'Vega20c', 'Wistia', 'YlGn', 'YlGnBu', 'YlOrBr', 'YlOrRd', 'afmhot', 'autumn', 'binary', 'bone', 'brg', 'bwr', 'cool', 'coolwarm', 'copper', 'cubehelix', 'flag', 'gist_earth', 'gist_gray', 'gist_heat', 'gist_ncar', 'gist_rainbow', 'gist_stern', 'gist_yarg', 'gnuplot', 'gnuplot2', 'gray', 'hot', 'hsv', 'jet', 'nipy_spectral', 'ocean', 'pink', 'prism', 'rainbow', 'seismic', 'spectral', 'spring', 'summer', 'tab10', 'tab20', 'tab20b', 'tab20c', 'terrain', 'winter'), value='viridis'), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#give our anatomical image a variable name\n",
    "mydata='./osfshare/sub-97-T1w_defaced.nii.gz'\n",
    "\n",
    "#what exactly does each do below\n",
    "#mydata_widget = NiftiWidget(mydata)\n",
    "mydata_widget.nifti_plotter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images we have been viewing are high-resolution images using [T1 contrast](http://www.med.harvard.edu/aanlib/basicsMR.html), and the image format is [nifti](https://brainder.org/2012/09/23/the-nifti-file-format/). As you see above, the image is 3-dimensional and can be opened in 3 orthogonal views simultaneously; coronal, sagittal and axial views.The viewer above is useful for getting familiar with the x,y,and z \"planes\" of the iamge. Every 3-dimensional pixel in the image (called a voxel) has a unique x,y,z coordinate.  <br>\n",
    "<br>\n",
    "It is also common to refer to different parts of the image by their location in neuroanatomical terms: L, R, A, P, S, I (Left, Right, Anterior=front, Posterior=back, Superior=top, Inferior=bottom). We will see that some viewers will show us these labels and \"Left\" in the image is not always the \"Left\" part of the head! <br>\n",
    "<br>\n",
    "It may be obvious, but also note that in these images we cannot see at the level of single neurons. The anatomy we focus on with MRI is from a \"macroscopic\" view. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FSL package viewer\n",
    "\n",
    "A very common neuroimaging package is FSL. FSL has viewer that we can open in the bash terminal, and it has some very helpful features for understanding more about images.\n",
    "\n",
    "__We'll first see how to:__ <br>\n",
    "<br>\n",
    "__(a) learn basic information about the image attributes__ <br> \n",
    "-What kind of image is this <br>\n",
    "-What is the voxel size <br>\n",
    "-Others? <br>\n",
    "<br>\n",
    "__(b) look at the image histogram.__ <br>\n",
    "An image histogram shows the distribution of all the intensity values that make up the image. Looking at these is helpful to see what intensities are generally “brain” versus background and to get comfortable with the idea that images are a collection of voxels with different intensity values (e.g., see Handbook Figure 2.1, p14). The intensity values mean something about what tissue we *think* we’re seeing based on the physics of MRI. It is also helpful to remember that the basis of most image processing tools is signal processing and statistics on these intensity values to improve the detection of the \"signal\" we're interested in (e.g., brain structure or activity) from noise (e.g., motion). <br>\n",
    "-What is the range of values in our sub-97-T1w_defaced.nii.gz image <br>\n",
    "-Is the histogram multi-modal? What would that tell us?  <br>\n",
    "-What are the typical values for gray matter, white matter, and CSF? <br>\n",
    "<br>\n",
    "__(c) look at our functional image and its timeseries.__ <br>\n",
    "-Compare and contrast the image intensities of white matter and gray matter from the structural with the functional image. <br>\n",
    "-Show the image timeseries <br>\n",
    "-Show the movie of volumes over time, how many dimensions is the file? <br>\n",
    "-Compare the spatial resolution of the structural and functional <br>\n",
    "-Can we overlay the functional on top of the structural image? <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "#the viewer version we'll use is called fslview, you can open it without opening an image by typing fslview\n",
    "fslview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "\n",
    "The black lines represent the time series data from the grid of voxels within the green square cursor at the center of the cross-hair in the brain viewer window. To review, fMRI data are a set of intensity values for 3D images of the brain that are collected over time. For instance, imagine the structural image above (and associated intensity values you see in the magnified matrix) now collected multiple times over the course of 5 minutes. The result is each voxel has not only one intensity value, rather each voxel now has a timeseries of intensity values. In functional MRI these intensity values reflect the BOLD response, which we'll talk about more next week. For now, it may be helpful to see this idea visualized again from the Aguirre 2014 paper we read:\n",
    "\n",
    "MV: below is too complex for the first lab; our goal for this one is to have them open a functional image in a viewer and play around with seeing the resolution and udnerstanding what a timeseries looks like from different voxels. This could be problem 3. I like afni viewer for this because it introduces them to a common viewer and the grid is nice for learning how the timeseries changes as you move around the brain.\n",
    "\n",
    "Then have problem 4 be a simple coding challenge to get warmed up on basic commands we will use a lot. May include a problem for setting up their github configuration to their username on their virtual machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "afni /home/brain/nilearn_data/haxby2001/subj2/bold.nii.gz &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mv suggests we hold this after we talk about what masks are \n",
    "\n",
    "import nibabel as nib\n",
    "from nilearn import input_data\n",
    "# load the face mask from the haxby dataset into python\n",
    "face_mask_img = nib.load(haxby_dataset.mask_face[0])\n",
    "# load the house mask from the haxby dataset into python\n",
    "house_mask_img = nib.load(haxby_dataset.mask_house[0])\n",
    "# concatonate the masks to make 4D image (each 3D image has one mask)\n",
    "haxby_maps = nib.funcs.concat_images([face_mask_img, house_mask_img])\n",
    "\n",
    "# nilearn utility to apply the masks to a dataset\n",
    "masker = input_data.NiftiMapsMasker(\n",
    "    haxby_maps, resampling_target=\"data\", detrend=True, memory='nilearn_cache', memory_level=10, verbose=2)\n",
    "# get the timeseries \n",
    "time_series = masker.fit_transform(func_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "haxby_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot the two time series\n",
    "face_plot, = plt.plot(time_series[:,0], label='face')\n",
    "house_plot, = plt.plot(time_series[:,1], label='house')\n",
    "plt.legend([face_plot, house_plot], ['face', 'house'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
